{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35536f6-166c-4b89-8136-96417db5be30",
   "metadata": {
    "id": "b35536f6-166c-4b89-8136-96417db5be30"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 8 [Assessment]:** RAG Evaluation</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Welcome to the last notebook of the course! In the previous notebook, you integrated a vector store solution into a RAG pipeline! In this notebook, you will take that same pipeline and evaluate it using numerical RAG evaluation techniques incorporating LLM-as-a-Judge metrics!\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Learn how to integrate the techniques from prior notebooks to numerically approximate the goodness of your RAG pipeline.\n",
    "\n",
    "- **Final Exercice**: ***By working through this notebook in the Course Environment,* you will be able to submit the coding component of the course!**\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- As you go along, remember what our metrics actually represent. Should our pipeline pass these objectives? Is our judge LLM sufficient for evaluating the pipeline? Does a particular metric even matter for our use case?\n",
    "- If we left the vectorstore-as-a-memory component in our chain, do you think it would still pass the evaluation? Additionally, is the evaluation useful for assessing vectorstore-as-a-memory performance? \n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook Source:**\n",
    "\n",
    "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://www.nvidia.com/en-sg/training/instructor-led-workshops/building-rag-agents-with-llms/). If sharing this material, please give credit and link back to the original course.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "w_A3rZOrIeQD",
   "metadata": {
    "id": "w_A3rZOrIeQD"
   },
   "outputs": [],
   "source": [
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu ragas\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "norm_style = Style(bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "pprint2 = partial(console.print, style=norm_style)\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEgV11oZmJGg",
   "metadata": {
    "id": "zEgV11oZmJGg"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Pre-Release Evaluation\n",
    "\n",
    "In our previous notebook, we successfully combined several concepts to create a document chatbot with the aim of responsive and informative interactions. However, the diversity of user interactions necessitates comprehensive testing to truly understand the chatbot's performance. Thorough testing in varied scenarios is crucial to ensure that the system is not only robust and versatile but also aligns with user and provider expectations.\n",
    "\n",
    "After defining your chatbot's roles and implementing the necessary features, evaluating it becomes a multi-stage process:\n",
    "\n",
    "- **Typical Use Inspection:** Start by testing scenarios most relevant to your use case. See if your chatbot can reliably navigate discussions with limited human intervention.\n",
    "\n",
    "    - Additionally, identify limitations or compartments that should be redirected to a human for inspection/supervision (i.e., human swap-in to confirm transactions or perform sensitive navigation) and implement those options.\n",
    "\n",
    "- **Edge Case Inspection:** Explore the boundaries of typical use, identifying how the chatbot handles less common but plausible scenarios.\n",
    "\n",
    "    - Before any public release, assess critical boundary conditions that could pose liability risks, such as the potential generation of inappropriate content.\n",
    "\n",
    "    - Implement well-tested guardrails on all outputs (and possibly inputs) to limit undesired interactions and redirect users into predictable conversation flows.\n",
    "\n",
    "- **Progressive Rollout:** Rolling out your model to a limited audience (first internal, then [A/B](https://en.wikipedia.org/wiki/A/B_testing)) and implement analytics features like usage analytics dashboards and feedback avenues (flag/like/dislike/etc).\n",
    "\n",
    "Of these three steps, the first two can be done by a small team or an individual and should be iterated on as part of the development process. Unfortunately, this needs to be done frequently and can be prone to human error. **Luckily for us, LLMs can be used to help out with LLM-as-a-Judge formulations!**\n",
    "\n",
    "*(Yeah, this probably isn't surprising by now. LLMs being strong is why this course is here...).*\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** LLM-as-a-Judge Formulation\n",
    "\n",
    "In the realm of conversational AI, using LLMs as evaluators or 'judges' has emerged as a useful approach for configurable automatic testing of natural language task performance:\n",
    "\n",
    "- An LLM can simulate a range of interaction scenarios and generate synthetic data, allowing an evaluation developer to generate targeted inputs to eliciting a range of behaviors from your chatbot.\n",
    "\n",
    "- The chatbot's correspondence/retrieval on the synthetic data can be evaluated or parsed by an LLM and a consistent output format such as \"Pass\"/\"Fail\", similarity, or extraction can be enforced.\n",
    "\n",
    "- Many such results can be aggregated and a metric can be derived which explains something like \"% of passing evaluations\", \"average number of relevant details from the sources\", \"average cosine similarity\", etc.\n",
    "\n",
    "This idea of using LLMs to test out and quantify chatbot quality, known as [**\"LLM-as-a-Judge,\"**](https://arxiv.org/abs/2306.05685) allows for easy test specifications that align closely with human judgment and can be fine-tuned and replicated at scale.\n",
    "\n",
    "**There are several popular frameworks for off-the-shelf judge formulations including:**\n",
    "- [**RAGAs (short for RAG Assessment)**](https://docs.ragas.io/en/stable/), which offers a suite of great starting points for your own evaluation efforts.\n",
    "- [**LangChain Evaluators**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/), which are similar first-party options with many implicitly-constructible agents.\n",
    "\n",
    "Instead of using the chains as-is, we will instead expand on the ideas and evaluate our system with a more custom solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDDNaBA9N3XM",
   "metadata": {
    "id": "fDDNaBA9N3XM"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3: [Assessment Prep]** Pairwise Evaluator\n",
    "\n",
    "The following exercise will flesh out a custom implementation of a simplified [LangChain Pairwise String Evaluator](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/comparison/pairwise_string/). \n",
    "\n",
    "**To prepare for our RAG chain evaluation, we will need to:**\n",
    "\n",
    "- Pull in our document index (the one we saved in the previous notebook).\n",
    "- Recreate our RAG pipeline of choice.\n",
    "\n",
    "**We will specifically be implementing a judge formulation with the following steps:**\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "**The chain should be a simple but powerful process that tests for the following objective:**\n",
    "\n",
    "> ***Does my RAG chain outperform a narrow chatbot with limited document access.***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This will be the system used for the final evaluation!** To see how this system is integrated into the autograder, please check out the implementation in [`frontend/server_app.py`](frontend/server_app.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bh8jaOqak0f",
   "metadata": {
    "id": "1bh8jaOqak0f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 1:** Pull In Your Document Retrieval Index\n",
    "\n",
    "For this exercise, you will pull in the `docstore_index` file you created as part of your earlier notebook. The following cell should be able to load in the store as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tlE7a2lseLOy",
   "metadata": {
    "id": "tlE7a2lseLOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Constructed aggregate docstore with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">238</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> chunks</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mConstructed aggregate docstore with \u001b[0m\u001b[1;36m238\u001b[0m\u001b[1;38;2;118;185;0m chunks\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "\n",
      "Summary: Large pre-trained language models have been shown to store factual knowledge\n",
      "in their parameters, and achieve state-of-the-art results when fine-tuned on\n",
      "downstream NLP tasks. However, their ability to access and precisely manipulate\n",
      "knowledge is still limited, and hence on knowledge-intensive tasks, their\n",
      "performance lags behind task-specific architectures. Additionally, providing\n",
      "provenance for their decisions and updating their world knowledge remain open\n",
      "research problems. Pre-trained models with a differentiable access mechanism to\n",
      "explicit non-parametric memory can overcome this issue, but have so far been\n",
      "only investigated for extractive downstream tasks. We explore a general-purpose\n",
      "fine-tuning recipe for retrieval-augmented generation (RAG) -- models which\n",
      "combine pre-trained parametric and non-parametric memory for language\n",
      "generation. We introduce RAG models where the parametric memory is a\n",
      "pre-trained seq2seq model and the non-parametric memory is a dense vector index\n",
      "of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\n",
      "formulations, one which conditions on the same retrieved passages across the\n",
      "whole generated sequence, the other can use different passages per token. We\n",
      "fine-tune and evaluate our models on a wide range of knowledge-intensive NLP\n",
      "tasks and set the state-of-the-art on three open domain QA tasks, outperforming\n",
      "parametric seq2seq models and task-specific retrieve-and-extract architectures.\n",
      "For language generation tasks, we find that RAG models generate more specific,\n",
      "diverse and factual language than a state-of-the-art parametric-only seq2seq\n",
      "baseline.\n",
      "\n",
      "Page Body: .S.\\nRAG-T It\\u2019s the only U.S. state named for a U.S. president\\nRAG-S It\\u2019s the state where you\\u2019ll \\ufb01nd Mount Rainier National Park\\nThe Divine\\nComedy\\nBART\\n*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\\nRAG-T Dante\\u2019s \\\"Inferno\\\" is the \\ufb01rst part of this epic poem\\nRAG-S This 14th century work is divided into 3 sections: \\\"Inferno\\\", \\\"Purgatorio\\\" & \\\"Paradiso\\\"\\nFor 2-way classi\\ufb01cation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\\nevidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved\\nby RAG and gold evidence annotations\n"
     ]
    }
   ],
   "source": [
    "## Make sure you have docstore_index.tgz in your working directory\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/embed-qa-4\", truncate=\"END\")\n",
    "\n",
    "!tar xzvf docstore_index.tgz\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "def format_chunk(doc):\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "    )\n",
    "\n",
    "## This printout just confirms that your store has been retrieved\n",
    "pprint(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "pprint(f\"Sample Chunk:\")\n",
    "print(format_chunk(docs[len(docs)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dib0F-t2N4LJ",
   "metadata": {
    "id": "dib0F-t2N4LJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2: [Exercise]** Pull In Your RAG Chain\n",
    "\n",
    "Now that we have our index, we can recreate the RAG agent from the previous notebook! \n",
    "\n",
    "**Key Modifications:**\n",
    "- To keep things simple, feel free to disregard the vectorstore-as-a-memory component. Incorporating it will require some more overhead and will make the exercise a bit more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "XBi6Y8b8aXd2",
   "metadata": {
    "id": "XBi6Y8b8aXd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to know something interesting? Did you know that the largest snowflake ever recorded was 15 inches wide and 8 inches thick? It fell in Montana in 1887! Isn't that mind-blowing?\n",
      "\n",
      "( Source: Document Retrieval from various sources )"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "llm = instruct_llm | StrOutputParser()\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "    \" The following information may be useful for your response: \"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    ")\n",
    "\n",
    "def output_puller(inputs):\n",
    "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
    "    if isinstance(inputs, dict):\n",
    "        inputs = [inputs]\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "#####################################################################\n",
    "## TODO: Pull in your desired RAG Chain. Memory not necessary\n",
    "\n",
    "## Chain 1 Specs: \"Hello World\" -> retrieval_chain \n",
    "##   -> {'input': <str>, 'context' : <str>}\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)  ## GIVEN\n",
    "context_getter = itemgetter('input') | docstore.as_retriever() | long_reorder | docs2str  ## TODO\n",
    "retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter})\n",
    "\n",
    "## Chain 2 Specs: retrieval_chain -> generator_chain \n",
    "##   -> {\"output\" : <str>, ...} -> output_puller\n",
    "generator_chain = chat_prompt | llm  ## TODO\n",
    "generator_chain = {'output' : generator_chain} | RunnableLambda(output_puller)  ## GIVEN\n",
    "\n",
    "## END TODO\n",
    "#####################################################################\n",
    "\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
    "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Step 3:** Generating Synthetic Question-Answer Pairs\n",
    "\n",
    "In this section, we can implement the first few part of our evaluation routine:\n",
    "\n",
    "- **Sample the RAG agent document pool to find two document chunks.**\n",
    "- **Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.**\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ymzuX-DSNvL6",
   "metadata": {
    "id": "ymzuX-DSNvL6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How does Mistral 7B achieve its superior performance in code, mathematics, and reasoning benchmarks, and </span>\n",
       "<span style=\"font-weight: bold\">which specific attention mechanisms enable this performance?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How does Mistral 7B achieve its superior performance in code, mathematics, and reasoning benchmarks, and \u001b[0m\n",
       "\u001b[1mwhich specific attention mechanisms enable this performance?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: Mistral 7B leverages a combination of grouped-query attention (GQA) for faster inference and sliding window</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost, allowing it to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">outperform Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 13B and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 34B in the aforementioned tasks.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: Mistral 7B leverages a combination of grouped-query attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mGQA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m for faster inference and sliding window\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mattention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSWA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m to effectively handle sequences of arbitrary length with a reduced inference cost, allowing it to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0moutperform Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m 13B and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m 34B in the aforementioned tasks.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: Can large language models (LLMs) trained with human preferences serve as effective substitutes for humans</span>\n",
       "<span style=\"font-weight: bold\">in evaluating other models, and what limitations and biases do they introduce?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: Can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m trained with human preferences serve as effective substitutes for humans\u001b[0m\n",
       "\u001b[1min evaluating other models, and what limitations and biases do they introduce?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: According to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, using strong LLMs as </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">judges to evaluate other models can achieve over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% agreement with human preferences, suggesting that LLMs can </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">serve as effective substitutes for humans in this context. However, the paper also highlights limitations and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">biases of LLM-judges, including position, verbosity, and self-enhancement biases, as well as limited reasoning </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ability, which can be mitigated with careful design and evaluation. This suggests that while LLMs can be useful in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">this role, they should be used with consideration of their limitations and potential biases.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: According to the paper \u001b[0m\u001b[32m\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"\u001b[0m\u001b[1;38;2;118;185;0m, using strong LLMs as \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mjudges to evaluate other models can achieve over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% agreement with human preferences, suggesting that LLMs can \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mserve as effective substitutes for humans in this context. However, the paper also highlights limitations and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbiases of LLM-judges, including position, verbosity, and self-enhancement biases, as well as limited reasoning \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mability, which can be mitigated with careful design and evaluation. This suggests that while LLMs can be useful in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthis role, they should be used with consideration of their limitations and potential biases.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: Can a strong large language model (LLM) serve as a reliable judge for evaluating chat assistants on </span>\n",
       "<span style=\"font-weight: bold\">open-ended questions, and are there potential biases or limitations in such a setup?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: Can a strong large language model \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLM\u001b[0m\u001b[1m)\u001b[0m\u001b[1m serve as a reliable judge for evaluating chat assistants on \u001b[0m\n",
       "\u001b[1mopen-ended questions, and are there potential biases or limitations in such a setup?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The answer is yes, a strong LLM like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> can match human preferences with over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% agreement on chat </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">assistant evaluations, according to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. However, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">potential biases or limitations exist, such as position, verbosity, and self-enhancement biases, as well as limited</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reasoning ability, which the paper proposes solutions to mitigate. The Transformer architecture, proposed in the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Attention Is All You Need\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, has been shown to be capable of handling complex tasks like language translation</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and constituency parsing, which may indirectly inform the design of LLMs and their use as judges.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The answer is yes, a strong LLM like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m can match human preferences with over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% agreement on chat \u001b[0m\n",
       "\u001b[1;38;2;118;185;0massistant evaluations, according to the paper \u001b[0m\u001b[32m\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"\u001b[0m\u001b[1;38;2;118;185;0m. However, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpotential biases or limitations exist, such as position, verbosity, and self-enhancement biases, as well as limited\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreasoning ability, which the paper proposes solutions to mitigate. The Transformer architecture, proposed in the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpaper \u001b[0m\u001b[32m\"Attention Is All You Need\"\u001b[0m\u001b[1;38;2;118;185;0m, has been shown to be capable of handling complex tasks like language translation\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand constituency parsing, which may indirectly inform the design of LLMs and their use as judges.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_questions = 3\n",
    "synth_questions = []\n",
    "synth_answers = []\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', 'INPUT: {input}')])\n",
    "\n",
    "for i in range(num_questions):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "        \" DO NOT SAY: \\\"Here is an interesting question pair\\\" or similar. FOLLOW FORMAT!\"\n",
    "    )\n",
    "    usr_msg = (\n",
    "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
    "        f\"Document2: {format_chunk(doc2)}\"\n",
    "    )\n",
    "\n",
    "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
    "    synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
    "    synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
    "    pprint2(f\"QA Pair {i+1}\")\n",
    "    pprint2(synth_questions[-1])\n",
    "    pprint(synth_answers[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5Q-3X4vS98P",
   "metadata": {
    "id": "c5Q-3X4vS98P"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 4:** Answer The Synthetic Questions\n",
    "\n",
    "In this section, we can implement the third part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- **Use the RAG agent to generate its own answer.**\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7T3GSwhZPHjF",
   "metadata": {
    "id": "7T3GSwhZPHjF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"font-weight: bold\">Question: How does Mistral 7B achieve its superior performance in code, mathematics, and reasoning benchmarks, and </span>\n",
       "<span style=\"font-weight: bold\">which specific attention mechanisms enable this performance?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\u001b[1mQuestion: How does Mistral 7B achieve its superior performance in code, mathematics, and reasoning benchmarks, and \u001b[0m\n",
       "\u001b[1mwhich specific attention mechanisms enable this performance?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Mistral 7B achieves its superior performance in code, mathematics, and reasoning benchmarks by </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">leveraging two attention mechanisms: grouped-query attention (GQA) and sliding window attention (SWA). According to</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the authors, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Mistral 7B leverages grouped-query attention (GQA) [1], and sliding window attention (SWA) [6, 3]. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications.\"</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">GQA and SWA work together to effectively handle sequences of arbitrary length with a reduced inference cost. This </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">allows Mistral 7B to approach the coding performance of Code-Llama 7B without sacrificing performance on non-code </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">related benchmarks.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Is there anything else I can help you with?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Mistral 7B achieves its superior performance in code, mathematics, and reasoning benchmarks by \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mleveraging two attention mechanisms: grouped-query attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mGQA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m and sliding window attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSWA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. According to\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe authors, \u001b[0m\u001b[32m\"Mistral 7B leverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, and sliding window attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m6, 3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. \u001b[0m\n",
       "\u001b[32mGQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, \u001b[0m\n",
       "\u001b[32mallowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications.\"\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mGQA and SWA work together to effectively handle sequences of arbitrary length with a reduced inference cost. This \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mallows Mistral 7B to approach the coding performance of Code-Llama 7B without sacrificing performance on non-code \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrelated benchmarks.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIs there anything else I can help you with?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"font-weight: bold\">Question: Can large language models (LLMs) trained with human preferences serve as effective substitutes for humans</span>\n",
       "<span style=\"font-weight: bold\">in evaluating other models, and what limitations and biases do they introduce?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\u001b[1mQuestion: Can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m trained with human preferences serve as effective substitutes for humans\u001b[0m\n",
       "\u001b[1min evaluating other models, and what limitations and biases do they introduce?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: According to the study </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> large language models </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(LLMs) can indeed serve as effective substitutes for humans in evaluating other models, at least for certain tasks.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The study found that strong LLMs can achieve an agreement rate of over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">%, on par with the level of agreement </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">among human experts.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">However, the study also highlights some limitations and biases that LLMs introduce when used as evaluators. These </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">include:</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Position bias: LLMs tend to favor responses that appear at the top of the ranking, rather than considering a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">broader range of responses.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Verbosity bias: LLMs may favor longer responses over shorter ones, which can lead to over-inclusion of irrelevant</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">information.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Self-enhancement bias: LLMs may be more likely to evaluate themselves more favorably than human evaluators would.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Limited reasoning ability: LLMs may struggle with tasks that require complex reasoning or critical thinking.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Despite these limitations, the study suggests that with modifications to the default prompt, LLMs can also evaluate</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">metrics such as honesty and harmlessness in chat assistants.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, while LLMs have the potential to be effective substitutes for humans in evaluating other models, it's </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">essential to addresses the identified biases and limitations to ensure that their evaluations are accurate and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reliable.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Would you like me to elaborate on any of these points or provide further context?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: According to the study \u001b[0m\u001b[32m\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,\"\u001b[0m\u001b[1;38;2;118;185;0m large language models \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m can indeed serve as effective substitutes for humans in evaluating other models, at least for certain tasks.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mThe study found that strong LLMs can achieve an agreement rate of over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m%, on par with the level of agreement \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mamong human experts.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mHowever, the study also highlights some limitations and biases that LLMs introduce when used as evaluators. These \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minclude:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m* Position bias: LLMs tend to favor responses that appear at the top of the ranking, rather than considering a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbroader range of responses.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m* Verbosity bias: LLMs may favor longer responses over shorter ones, which can lead to over-inclusion of irrelevant\u001b[0m\n",
       "\u001b[1;38;2;118;185;0minformation.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m* Self-enhancement bias: LLMs may be more likely to evaluate themselves more favorably than human evaluators would.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m* Limited reasoning ability: LLMs may struggle with tasks that require complex reasoning or critical thinking.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mDespite these limitations, the study suggests that with modifications to the default prompt, LLMs can also evaluate\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmetrics such as honesty and harmlessness in chat assistants.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOverall, while LLMs have the potential to be effective substitutes for humans in evaluating other models, it's \u001b[0m\n",
       "\u001b[1;38;2;118;185;0messential to addresses the identified biases and limitations to ensure that their evaluations are accurate and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreliable.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mWould you like me to elaborate on any of these points or provide further context?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "<span style=\"font-weight: bold\">Question: Can a strong large language model (LLM) serve as a reliable judge for evaluating chat assistants on </span>\n",
       "<span style=\"font-weight: bold\">open-ended questions, and are there potential biases or limitations in such a setup?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\u001b[1mQuestion: Can a strong large language model \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLM\u001b[0m\u001b[1m)\u001b[0m\u001b[1m serve as a reliable judge for evaluating chat assistants on \u001b[0m\n",
       "\u001b[1mopen-ended questions, and are there potential biases or limitations in such a setup?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The use of strong large language models (LLMs) as judges for evaluating chat assistants on open-ended </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">questions has been a topic of interest. Research by Lianmin Zheng and his team, as published in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLM-as-a-Judge with MT-Bench and Chatbot Arena\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], suggests that using LLMs as judges can be a highly feasible </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">approach to approximate human preferences.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">However, their study also identifies certain biases and limitations of LLM judges, such as position bias, verbosity</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">bias, and self-enhancement bias. Position bias, for instance, occurs when an LLM exhibits a propensity to favor </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">certain positions over others. This bias is not unique to LLMs and has been seen in human decision-making and other</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ML domains.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Table </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> in their paper shows examples of position bias among different LLM judges, highlighting the need to address</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">these biases. The study proposes solutions to mitigate some of these biases, including using human-in-the-loop </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dynamics and modifying the default prompt to evaluate honesty and harmlessness.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Interestingly, their results indicate that using LLM-as-a-judge to approximate human preferences is highly feasible</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and could become a new standard in future benchmarks. Notably, their LLM-as-a-judge approach can automate and scale</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">platforms, such as DynaBench, which emphasizes dynamic data with human-in-the-loop.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While strong LLMs can achieve an agreement rate of over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% with human experts, it's essential to acknowledge the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">importance of addressing the limitations and biases in LLM judges to ensure their reliability.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">References:</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Zheng, L., Chiang, W., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">H., Gonzalez, J. E., Stoica, I. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Judging LLM-as-a- Judge with MT-Bench and Chatbot Arena.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, while a strong LLM can serve as a reliable judge for evaluating chat assistants on open-ended questions, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">there are potential biases and limitations in such a setup that need to be addressed. By understanding these </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">biases, researchers and developers can work towards creating more reliable and comprehensive evaluation frameworks </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for chat assistants.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The use of strong large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m as judges for evaluating chat assistants on open-ended \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mquestions has been a topic of interest. Research by Lianmin Zheng and his team, as published in \u001b[0m\u001b[32m\"Judging \u001b[0m\n",
       "\u001b[32mLLM-as-a-Judge with MT-Bench and Chatbot Arena\"\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, suggests that using LLMs as judges can be a highly feasible \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mapproach to approximate human preferences.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mHowever, their study also identifies certain biases and limitations of LLM judges, such as position bias, verbosity\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbias, and self-enhancement bias. Position bias, for instance, occurs when an LLM exhibits a propensity to favor \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcertain positions over others. This bias is not unique to LLMs and has been seen in human decision-making and other\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mML domains.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTable \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m in their paper shows examples of position bias among different LLM judges, highlighting the need to address\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthese biases. The study proposes solutions to mitigate some of these biases, including using human-in-the-loop \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdynamics and modifying the default prompt to evaluate honesty and harmlessness.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mInterestingly, their results indicate that using LLM-as-a-judge to approximate human preferences is highly feasible\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand could become a new standard in future benchmarks. Notably, their LLM-as-a-judge approach can automate and scale\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mplatforms, such as DynaBench, which emphasizes dynamic data with human-in-the-loop.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mWhile strong LLMs can achieve an agreement rate of over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% with human experts, it's essential to acknowledge the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mimportance of addressing the limitations and biases in LLM judges to ensure their reliability.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mReferences:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Zheng, L., Chiang, W., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mH., Gonzalez, J. E., Stoica, I. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2023\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Judging LLM-as-a- Judge with MT-Bench and Chatbot Arena.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOverall, while a strong LLM can serve as a reliable judge for evaluating chat assistants on open-ended questions, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthere are potential biases and limitations in such a setup that need to be addressed. By understanding these \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbiases, researchers and developers can work towards creating more reliable and comprehensive evaluation frameworks \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfor chat assistants.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Generate some synthetic answers to the questions above.\n",
    "##   Try to use the same syntax as the cell above\n",
    "rag_answers = []\n",
    "for i, q in enumerate(synth_questions):\n",
    "    ## TODO: Compute the RAG Answer\n",
    "    rag_answer = rag_chain.invoke(q)\n",
    "    rag_answers += [rag_answer]\n",
    "    pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\")\n",
    "    pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ho5cnN_Xt_yr",
   "metadata": {
    "id": "Ho5cnN_Xt_yr"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 5:** Implement A Human Preference Metric\n",
    "\n",
    "In this section, we can implement the fourth part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- **Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"**\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sf6f2oFLuPtu",
   "metadata": {
    "id": "sf6f2oFLuPtu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How does Mistral 7B achieve its superior performance in code, mathematics, and reasoning </span>\n",
       "<span style=\"font-weight: bold\">benchmarks, and which specific attention mechanisms enable this performance?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How does Mistral 7B achieve its superior performance in code, mathematics, and reasoning \u001b[0m\n",
       "\u001b[1mbenchmarks, and which specific attention mechanisms enable this performance?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: Mistral 7B leverages a combination of grouped-query attention (GQA) for faster inference and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">allowing it to outperform Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 13B and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 34B in the aforementioned tasks.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: Mistral 7B leverages a combination of grouped-query attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mGQA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m for faster inference and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msliding window attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSWA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m to effectively handle sequences of arbitrary length with a reduced inference cost, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mallowing it to outperform Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m 13B and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m 34B in the aforementioned tasks.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Mistral 7B achieves its superior performance in code, mathematics, and reasoning benchmarks by </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">leveraging two attention mechanisms: grouped-query attention (GQA) and sliding window attention (SWA). According to</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the authors, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Mistral 7B leverages grouped-query attention (GQA) [1], and sliding window attention (SWA) [6, 3]. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications.\"</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">GQA and SWA work together to effectively handle sequences of arbitrary length with a reduced inference cost. This </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">allows Mistral 7B to approach the coding performance of Code-Llama 7B without sacrificing performance on non-code </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">related benchmarks.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Is there anything else I can help you with?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Mistral 7B achieves its superior performance in code, mathematics, and reasoning benchmarks by \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mleveraging two attention mechanisms: grouped-query attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mGQA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m and sliding window attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSWA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. According to\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe authors, \u001b[0m\u001b[32m\"Mistral 7B leverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, and sliding window attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m6, 3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. \u001b[0m\n",
       "\u001b[32mGQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, \u001b[0m\n",
       "\u001b[32mallowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications.\"\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mGQA and SWA work together to effectively handle sequences of arbitrary length with a reduced inference cost. This \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mallows Mistral 7B to approach the coding performance of Code-Llama 7B without sacrificing performance on non-code \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrelated benchmarks.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIs there anything else I can help you with?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">] Justification: The second answer attempts to answer the question, but provides </span>\n",
       "<span style=\"font-weight: bold\">additional context and conflicting information. The addition of </span><span style=\"color: #008000; text-decoration-color: #008000\">\"[1], [6, 3]\"</span><span style=\"font-weight: bold\"> in the first sentence is </span>\n",
       "<span style=\"font-weight: bold\">inconsistent, as it appears to be a reference list, but the text does not provide any corresponding references. </span>\n",
       "<span style=\"font-weight: bold\">Moreover, the statement </span><span style=\"color: #008000; text-decoration-color: #008000\">\"This allows Mistral 7B to approach the coding performance of Code-Llama 7B without </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sacrificing performance on non-code related benchmarks\"</span><span style=\"font-weight: bold\"> deviates from the original question, which asks about </span>\n",
       "<span style=\"font-weight: bold\">comparison to Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> 13B and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> 34B. The second answer also does not provide a clear and concise answer to </span>\n",
       "<span style=\"font-weight: bold\">the question, making it inferior to the first answer.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m Justification: The second answer attempts to answer the question, but provides \u001b[0m\n",
       "\u001b[1madditional context and conflicting information. The addition of \u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, \u001b[0m\u001b[32m[\u001b[0m\u001b[32m6, 3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\"\u001b[0m\u001b[1m in the first sentence is \u001b[0m\n",
       "\u001b[1minconsistent, as it appears to be a reference list, but the text does not provide any corresponding references. \u001b[0m\n",
       "\u001b[1mMoreover, the statement \u001b[0m\u001b[32m\"This allows Mistral 7B to approach the coding performance of Code-Llama 7B without \u001b[0m\n",
       "\u001b[32msacrificing performance on non-code related benchmarks\"\u001b[0m\u001b[1m deviates from the original question, which asks about \u001b[0m\n",
       "\u001b[1mcomparison to Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m 13B and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m 34B. The second answer also does not provide a clear and concise answer to \u001b[0m\n",
       "\u001b[1mthe question, making it inferior to the first answer.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: Can large language models (LLMs) trained with human preferences serve as effective substitutes </span>\n",
       "<span style=\"font-weight: bold\">for humans in evaluating other models, and what limitations and biases do they introduce?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: Can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m trained with human preferences serve as effective substitutes \u001b[0m\n",
       "\u001b[1mfor humans in evaluating other models, and what limitations and biases do they introduce?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: According to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, using strong</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">LLMs as judges to evaluate other models can achieve over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% agreement with human preferences, suggesting that LLMs</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">can serve as effective substitutes for humans in this context. However, the paper also highlights limitations and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">biases of LLM-judges, including position, verbosity, and self-enhancement biases, as well as limited reasoning </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ability, which can be mitigated with careful design and evaluation. This suggests that while LLMs can be useful in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">this role, they should be used with consideration of their limitations and potential biases.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: According to the paper \u001b[0m\u001b[32m\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"\u001b[0m\u001b[1;38;2;118;185;0m, using strong\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLLMs as judges to evaluate other models can achieve over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% agreement with human preferences, suggesting that LLMs\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcan serve as effective substitutes for humans in this context. However, the paper also highlights limitations and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbiases of LLM-judges, including position, verbosity, and self-enhancement biases, as well as limited reasoning \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mability, which can be mitigated with careful design and evaluation. This suggests that while LLMs can be useful in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthis role, they should be used with consideration of their limitations and potential biases.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: According to the study </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> large language models </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(LLMs) can indeed serve as effective substitutes for humans in evaluating other models, at least for certain tasks.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The study found that strong LLMs can achieve an agreement rate of over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">%, on par with the level of agreement </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">among human experts.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">However, the study also highlights some limitations and biases that LLMs introduce when used as evaluators. These </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">include:</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Position bias: LLMs tend to favor responses that appear at the top of the ranking, rather than considering a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">broader range of responses.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Verbosity bias: LLMs may favor longer responses over shorter ones, which can lead to over-inclusion of irrelevant</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">information.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Self-enhancement bias: LLMs may be more likely to evaluate themselves more favorably than human evaluators would.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Limited reasoning ability: LLMs may struggle with tasks that require complex reasoning or critical thinking.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Despite these limitations, the study suggests that with modifications to the default prompt, LLMs can also evaluate</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">metrics such as honesty and harmlessness in chat assistants.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, while LLMs have the potential to be effective substitutes for humans in evaluating other models, it's </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">essential to addresses the identified biases and limitations to ensure that their evaluations are accurate and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reliable.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Would you like me to elaborate on any of these points or provide further context?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: According to the study \u001b[0m\u001b[32m\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,\"\u001b[0m\u001b[1;38;2;118;185;0m large language models \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m can indeed serve as effective substitutes for humans in evaluating other models, at least for certain tasks.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mThe study found that strong LLMs can achieve an agreement rate of over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m%, on par with the level of agreement \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mamong human experts.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mHowever, the study also highlights some limitations and biases that LLMs introduce when used as evaluators. These \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minclude:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m* Position bias: LLMs tend to favor responses that appear at the top of the ranking, rather than considering a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbroader range of responses.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m* Verbosity bias: LLMs may favor longer responses over shorter ones, which can lead to over-inclusion of irrelevant\u001b[0m\n",
       "\u001b[1;38;2;118;185;0minformation.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m* Self-enhancement bias: LLMs may be more likely to evaluate themselves more favorably than human evaluators would.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m* Limited reasoning ability: LLMs may struggle with tasks that require complex reasoning or critical thinking.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mDespite these limitations, the study suggests that with modifications to the default prompt, LLMs can also evaluate\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmetrics such as honesty and harmlessness in chat assistants.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOverall, while LLMs have the potential to be effective substitutes for humans in evaluating other models, it's \u001b[0m\n",
       "\u001b[1;38;2;118;185;0messential to addresses the identified biases and limitations to ensure that their evaluations are accurate and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreliable.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mWould you like me to elaborate on any of these points or provide further context?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">. Justification</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">The second answer adds no new information or insights beyond the first answer, except for some minor word choice </span>\n",
       "<span style=\"font-weight: bold\">and sentence structure variations. In fact, the second answer's summary of the same paper and its findings is </span>\n",
       "<span style=\"font-weight: bold\">identical to the first answer's description. Further, the second answer provides almost the same list of </span>\n",
       "<span style=\"font-weight: bold\">limitations and biases of LLM-judges as the first answer, and the suggestions for mitigating these limitations are </span>\n",
       "<span style=\"font-weight: bold\">also the same.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Since there are no significant differences between the two answers, I rated the second answer as inconsistent with </span>\n",
       "<span style=\"font-weight: bold\">the first answer. Therefore, the score is </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m. Justification\u001b[0m\n",
       "\n",
       "\u001b[1mThe second answer adds no new information or insights beyond the first answer, except for some minor word choice \u001b[0m\n",
       "\u001b[1mand sentence structure variations. In fact, the second answer's summary of the same paper and its findings is \u001b[0m\n",
       "\u001b[1midentical to the first answer's description. Further, the second answer provides almost the same list of \u001b[0m\n",
       "\u001b[1mlimitations and biases of LLM-judges as the first answer, and the suggestions for mitigating these limitations are \u001b[0m\n",
       "\u001b[1malso the same.\u001b[0m\n",
       "\n",
       "\u001b[1mSince there are no significant differences between the two answers, I rated the second answer as inconsistent with \u001b[0m\n",
       "\u001b[1mthe first answer. Therefore, the score is \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: Can a strong large language model (LLM) serve as a reliable judge for evaluating chat </span>\n",
       "<span style=\"font-weight: bold\">assistants on open-ended questions, and are there potential biases or limitations in such a setup?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: Can a strong large language model \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLM\u001b[0m\u001b[1m)\u001b[0m\u001b[1m serve as a reliable judge for evaluating chat \u001b[0m\n",
       "\u001b[1massistants on open-ended questions, and are there potential biases or limitations in such a setup?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: The answer is yes, a strong LLM like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> can match human preferences with over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">agreement on chat assistant evaluations, according to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging LLM-as-a-Judge with MT-Bench and Chatbot </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arena\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. However, potential biases or limitations exist, such as position, verbosity, and self-enhancement biases, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">as well as limited reasoning ability, which the paper proposes solutions to mitigate. The Transformer architecture,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">proposed in the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Attention Is All You Need\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, has been shown to be capable of handling complex tasks like </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language translation and constituency parsing, which may indirectly inform the design of LLMs and their use as </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">judges.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: The answer is yes, a strong LLM like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m can match human preferences with over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% \u001b[0m\n",
       "\u001b[1;38;2;118;185;0magreement on chat assistant evaluations, according to the paper \u001b[0m\u001b[32m\"Judging LLM-as-a-Judge with MT-Bench and Chatbot \u001b[0m\n",
       "\u001b[32mArena\"\u001b[0m\u001b[1;38;2;118;185;0m. However, potential biases or limitations exist, such as position, verbosity, and self-enhancement biases, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mas well as limited reasoning ability, which the paper proposes solutions to mitigate. The Transformer architecture,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mproposed in the paper \u001b[0m\u001b[32m\"Attention Is All You Need\"\u001b[0m\u001b[1;38;2;118;185;0m, has been shown to be capable of handling complex tasks like \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage translation and constituency parsing, which may indirectly inform the design of LLMs and their use as \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mjudges.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The use of strong large language models (LLMs) as judges for evaluating chat assistants on open-ended </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">questions has been a topic of interest. Research by Lianmin Zheng and his team, as published in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLM-as-a-Judge with MT-Bench and Chatbot Arena\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], suggests that using LLMs as judges can be a highly feasible </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">approach to approximate human preferences.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">However, their study also identifies certain biases and limitations of LLM judges, such as position bias, verbosity</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">bias, and self-enhancement bias. Position bias, for instance, occurs when an LLM exhibits a propensity to favor </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">certain positions over others. This bias is not unique to LLMs and has been seen in human decision-making and other</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ML domains.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Table </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> in their paper shows examples of position bias among different LLM judges, highlighting the need to address</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">these biases. The study proposes solutions to mitigate some of these biases, including using human-in-the-loop </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dynamics and modifying the default prompt to evaluate honesty and harmlessness.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Interestingly, their results indicate that using LLM-as-a-judge to approximate human preferences is highly feasible</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and could become a new standard in future benchmarks. Notably, their LLM-as-a-judge approach can automate and scale</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">platforms, such as DynaBench, which emphasizes dynamic data with human-in-the-loop.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While strong LLMs can achieve an agreement rate of over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% with human experts, it's essential to acknowledge the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">importance of addressing the limitations and biases in LLM judges to ensure their reliability.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">References:</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Zheng, L., Chiang, W., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">H., Gonzalez, J. E., Stoica, I. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Judging LLM-as-a- Judge with MT-Bench and Chatbot Arena.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, while a strong LLM can serve as a reliable judge for evaluating chat assistants on open-ended questions, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">there are potential biases and limitations in such a setup that need to be addressed. By understanding these </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">biases, researchers and developers can work towards creating more reliable and comprehensive evaluation frameworks </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for chat assistants.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The use of strong large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m as judges for evaluating chat assistants on open-ended \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mquestions has been a topic of interest. Research by Lianmin Zheng and his team, as published in \u001b[0m\u001b[32m\"Judging \u001b[0m\n",
       "\u001b[32mLLM-as-a-Judge with MT-Bench and Chatbot Arena\"\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, suggests that using LLMs as judges can be a highly feasible \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mapproach to approximate human preferences.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mHowever, their study also identifies certain biases and limitations of LLM judges, such as position bias, verbosity\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbias, and self-enhancement bias. Position bias, for instance, occurs when an LLM exhibits a propensity to favor \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcertain positions over others. This bias is not unique to LLMs and has been seen in human decision-making and other\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mML domains.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTable \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m in their paper shows examples of position bias among different LLM judges, highlighting the need to address\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthese biases. The study proposes solutions to mitigate some of these biases, including using human-in-the-loop \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdynamics and modifying the default prompt to evaluate honesty and harmlessness.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mInterestingly, their results indicate that using LLM-as-a-judge to approximate human preferences is highly feasible\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand could become a new standard in future benchmarks. Notably, their LLM-as-a-judge approach can automate and scale\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mplatforms, such as DynaBench, which emphasizes dynamic data with human-in-the-loop.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mWhile strong LLMs can achieve an agreement rate of over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% with human experts, it's essential to acknowledge the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mimportance of addressing the limitations and biases in LLM judges to ensure their reliability.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mReferences:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Zheng, L., Chiang, W., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mH., Gonzalez, J. E., Stoica, I. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2023\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Judging LLM-as-a- Judge with MT-Bench and Chatbot Arena.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOverall, while a strong LLM can serve as a reliable judge for evaluating chat assistants on open-ended questions, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthere are potential biases and limitations in such a setup that need to be addressed. By understanding these \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbiases, researchers and developers can work towards creating more reliable and comprehensive evaluation frameworks \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfor chat assistants.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8</span><span style=\"font-weight: bold\"> Justification</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Although Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> is more detailed and provides concrete examples to support the claim, it primarily builds upon </span>\n",
       "<span style=\"font-weight: bold\">the information provided in the original answer (Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">). The references are consistent, and the new information </span>\n",
       "<span style=\"font-weight: bold\">is not necessarily a better or more definitive answer to the question.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Some criticisms of Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> include:</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">- While it provides more context and examples, it doesn't necessarily offer a new perspective or a more </span>\n",
       "<span style=\"font-weight: bold\">comprehensive explanation of the limitations of LLMs as judges.</span>\n",
       "<span style=\"font-weight: bold\">- The information provided (e.g., the description of biases and the proposed solutions) is not necessarily </span><span style=\"color: #008000; text-decoration-color: #008000\">\"better\"</span>\n",
       "<span style=\"font-weight: bold\">than the original answer; it's more of a repetition with some additional details.</span>\n",
       "<span style=\"font-weight: bold\">- Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> does not introduce any inconsistencies or new information that contradicts the original ground truth.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Therefore, the score is </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8</span><span style=\"font-weight: bold\">, indicating that Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> is consistent with the ground truth but does not offer a </span>\n",
       "<span style=\"font-weight: bold\">significant improvement over the original answer.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1m Justification\u001b[0m\n",
       "\n",
       "\u001b[1mAlthough Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m is more detailed and provides concrete examples to support the claim, it primarily builds upon \u001b[0m\n",
       "\u001b[1mthe information provided in the original answer \u001b[0m\u001b[1m(\u001b[0m\u001b[1mAnswer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m. The references are consistent, and the new information \u001b[0m\n",
       "\u001b[1mis not necessarily a better or more definitive answer to the question.\u001b[0m\n",
       "\n",
       "\u001b[1mSome criticisms of Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m include:\u001b[0m\n",
       "\n",
       "\u001b[1m- While it provides more context and examples, it doesn't necessarily offer a new perspective or a more \u001b[0m\n",
       "\u001b[1mcomprehensive explanation of the limitations of LLMs as judges.\u001b[0m\n",
       "\u001b[1m- The information provided \u001b[0m\u001b[1m(\u001b[0m\u001b[1me.g., the description of biases and the proposed solutions\u001b[0m\u001b[1m)\u001b[0m\u001b[1m is not necessarily \u001b[0m\u001b[32m\"better\"\u001b[0m\n",
       "\u001b[1mthan the original answer; it's more of a repetition with some additional details.\u001b[0m\n",
       "\u001b[1m- Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m does not introduce any inconsistencies or new information that contradicts the original ground truth.\u001b[0m\n",
       "\n",
       "\u001b[1mTherefore, the score is \u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1m, indicating that Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m is consistent with the ground truth but does not offer a \u001b[0m\n",
       "\u001b[1msignificant improvement over the original answer.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Adapt this prompt for whichever LLM you're actually interested in using. \n",
    "## If it's llama, maybe system message would be good?\n",
    "eval_prompt = ChatPromptTemplate.from_template(\"\"\"INSTRUCTION: \n",
    "Evaluate the following Question-Answer pair for human preference and consistency.\n",
    "Assume the first answer is a ground truth answer and has to be correct.\n",
    "Assume the second answer may or may not be true.\n",
    "[1] The second answer lies, does not answer the question, or is inferior to the first answer.\n",
    "[2] The second answer is better than the first and does not introduce any inconsistencies.\n",
    "\n",
    "Output Format:\n",
    "[Score] Justification\n",
    "\n",
    "{qa_trio}\n",
    "\n",
    "EVALUATION: \n",
    "\"\"\")\n",
    "\n",
    "pref_score = []\n",
    "\n",
    "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "    pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
    "\n",
    "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
    "    pref_score += [(eval_prompt | llm).invoke({'qa_trio': qa_trio})]\n",
    "    pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
    "    pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
    "    pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Congratulations! We now have an LLM system that reasons about our pipeline and tries to evaluate it!** Now that we have some judge results, we can simply aggregate the results and see how often our formulation was according to an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3L_q6fMH3i6_",
   "metadata": {
    "id": "3L_q6fMH3i6_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "pref_score = sum((\"[2]\" in score) for score in pref_score) / len(pref_score)\n",
    "print(f\"Preference Score: {pref_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80bf04-118d-44a2-a740-361a756a1d5f",
   "metadata": {
    "id": "cf80bf04-118d-44a2-a740-361a756a1d5f"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4:** Advanced Formulations\n",
    "\n",
    "The exercise above was meant to prepare you for the final assessment of the course and showcased a simple but effective evaluator chain. The objective and implementation details were provided for you, and the logic for using it probably makes sense now that you've seen it in action. \n",
    "\n",
    "With that being said, this metric was merely a product of us specifying:\n",
    "- **What kind of behavior is important for our pipeline to have?**\n",
    "- **What do we need to do in order to exhibit and evaluate this behavior?**\n",
    "\n",
    "From these two questions, we could have come up with plenty of other evaluation metrics that could have assessed different attributes, incorporated different evaluator chain techniques, and even required different pipeline organization strategies. Though far from an exhaustive list, some common formulations you will likely come across may include:\n",
    "\n",
    "- **Style Evaluation:** Some evaluation formulations can be as simple as \"let me ask some questions and see if the output feels desirable.\" This might be used to see whether a chatbot \"acts like it's supposed to\" based on a description provided to a judge LLM. We're using quotations since this kind of assessment can reasonably be achieved with nothing but prompt engineering and a while loop.\n",
    "\n",
    "- **Ground-Truth Evaluation:** In our chain, we used synthetic generation to create some random questions and answers using a sampling strategy, but in reality you may actually have some representative questions and answers that you need your chatbot to consistently get right! In this case, a modification of the exercise chain above should be implemented and closely monitored as you develop your pipelines.\n",
    "\n",
    "- **Retrieval/Augmentation Evaluation:** This course made many assumptions about what kinds of preprocessing and prompting steps would be good for your pipelines, and much of this was determined by experimentation. Factors such as document preprocessing, chunking strategies, model selection, and prompt specification all played important roles, so creating metrics to validate these decisions may be of interest. This kind of metric might require your pipeline to output your context chunks or may even rely solely on embedding similarity comparisons, so keep this in mind when trying to implement a chain that works with multiple evaluation strategies. Consider the [**RagasEvaluatorChain**](https://docs.ragas.io/en/stable/howtos/integrations/langchain.html) abstraction as a decent starting point for making an custom generalizable evaluation routine. \n",
    "\n",
    "- **Trajectory Evaluation:** Using more advanced agent formulations, you can implement multiple-query strategies that assume the presence of conversational memory. With this, you can implement an evaluation agent which can:\n",
    "    - Ask a series of questions in order to evaluate how well the agent is able to adapt and cater to the scenario. This kind of system generally considers a series of correspondence and aims to tease out and evaluate a \"trajectory\" of how the agent navigated the conversation. The [**LangChain Trajectory Evaluators documentation**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/trajectory/) is a good starting point.\n",
    "    - Alternatively, you could also implement an evaluation agent that tries to achieve objectives by interacting with the chatbot. Such an agent can output whether they were able to navigate to their solution in a natural manner, and can even be used to generate a report about the percieved performance. The [**LangChain Agents documentation**](https://python.langchain.com/v0.1/docs/modules/agents/) is a good starting point!\n",
    "\n",
    "<br>\n",
    "\n",
    "At the end of the day, just make sure to use the tools you have at your disposal appropriately. By this point in the course, you should already be well-acquainted with the LLM core value propositions: **They're powerful, scalable, predictable, controllable, and orchestratable... but will act unpredictably when you just expect them to work by default.** Assess your needs, formulate and validate your pipelines, give enough information, and add as much control as you can to make your system work consistently, efficiently, and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faee2c-e534-4c89-91ae-45c37835dba5",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5: [Assessment]** Evaluating For Credit\n",
    "\n",
    "Welcome to the last exercise of the course! Hopefully you've enjoyed the material and are ready to actually get credit for these notebooks! For this part:\n",
    "\n",
    "- **Make sure you're in the course environment**\n",
    "- **Make sure `docstore_index/` has been uploaded to the course environment...**\n",
    "    - **...and contains [at least one Arxiv paper](https://arxiv.org/search/advanced) which has been updated recently.**\n",
    "- **Make sure you don't have some old session of [`09_langserve.ipynb`](09_langserve.ipynb) already occupying the port. Your assessment requires you to implement the new `/retriever` and `/generator` endpoints!!**\n",
    "\n",
    "**Objective:** On launch, [**`frontend/frontend_block.py`**](frontend/frontend_block.py) had several lines of code which trigger the course pass condition. Your objective is to invoke that series of commands by using your pipeline to pass the **Evaluation** check! Recall [`09_langserve.ipynb`](09_langserve.ipynb) and use it as a starting example! As a recommendation, consider duplicating it so that you can keep the original as an authoritative reference. \n",
    "\n",
    "**Once Finished:** While your course environment is still open, please navigate back to your course environment launcher area and click the **\"Assess Task\"** button! After that, you're all done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e300ed-951c-4006-ac54-cbbd41251707",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':8090';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0",
   "metadata": {
    "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## <font color=\"#76b900\">**Congratulations On Completing The Course**</font>\n",
    "\n",
    "Hopefully this course was not only exciting and challenging, but also adequately prepared you for work on the cutting edge of LLM and RAG system development! Going forward, you should have the skills necessary to tackle industry-level challenges and explore RAG deployment with open-source models and frameworks.\n",
    "\n",
    "**Some NVIDIA-specific releases related to this that you may find interesting include:**\n",
    "- [**NVIDIA NIM**](https://www.nvidia.com/en-us/ai/), which offers microservice spinup routines that can be deployed on local compute.\n",
    "- [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) is the current recommended framework for deploying GPU-accelerated LLM model engines in production settings.\n",
    "- [**NVIDIA's Generative AI Examples Repo**](https://github.com/NVIDIA/GenerativeAIExamples), which includes the current canonical microservice example application and will be updated with new resources as new production workflows get released.\n",
    "- [**The Knowledge-Based Chatbot Technical Brief**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief) which discusses additional publicly-accessible details on productionalizing RAG systems.\n",
    "\n",
    "**Additionally, some key topics you may be interested in delving more into include:**\n",
    "- [**LlamaIndex**](https://www.llamaindex.ai/), which has strong components that can augment and occasionally improve upon the LangChain RAG features.\n",
    "- [**LangSmith**](https://docs.smith.langchain.com/), an upcoming agent productionalization service offered by LangChain.\n",
    "- [**Gradio**](https://www.gradio.app/), though touched on in the course, has many more interface options which will be worth investigating. For inspiration, consider checking out [**HuggingFace Spaces**](https://huggingface.co/spaces) for examples.\n",
    "- [**LangGraph**](https://python.langchain.com/docs/langgraph/) is a framework for graph-based LLM orchestration, and is a natural next step forward for those interested in [multi-agent workflows](https://blog.langchain.dev/langgraph-multi-agent-workflows/).\n",
    "- [**DSPy**](https://github.com/stanfordnlp/dspy), a flow engineering framework that allows you to optimize LLM orchestration pipelines based on empirical performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035451c9-ed12-4bc3-b468-04db5c399e03",
   "metadata": {
    "id": "035451c9-ed12-4bc3-b468-04db5c399e03"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
